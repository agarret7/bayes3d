{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fef143-f8d4-4078-9386-deeff2fd80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import imageio\n",
    "import bayes3d as b\n",
    "from tqdm import tqdm\n",
    "import pytorch3d.transforms\n",
    "import jax.numpy as jnp\n",
    "import nvdiffrast.torch as dr\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae0d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter           = 10000\n",
    "repeats            = 1\n",
    "log_interval       = 10\n",
    "display_interval   = None\n",
    "display_res        = 512\n",
    "lr_base            = 1e-3\n",
    "lr_falloff         = 1.0\n",
    "nr_base            = 1.0\n",
    "nr_falloff         = 1e-4\n",
    "grad_phase_start   = 0.5\n",
    "resolution         = 128\n",
    "out_dir            = None\n",
    "log_fn             = None\n",
    "mp4save_interval   = None\n",
    "mp4save_fn         = None\n",
    "use_opengl         = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce3703-36df-4d82-bebc-751bb974c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(x=0.1, n=1.0, f=50.0):\n",
    "    return np.array([[n/x,    0,            0,              0],\n",
    "                     [  0,  n/x,            0,              0],\n",
    "                     [  0,    0, -(f+n)/(f-n), -(2*f*n)/(f-n)],\n",
    "                     [  0,    0,           -1,              0]]).astype(np.float32)\n",
    "def translate(x, y, z):\n",
    "    return np.array([[1, 0, 0, x],\n",
    "                     [0, 1, 0, y],\n",
    "                     [0, 0, 1, z],\n",
    "                     [0, 0, 0, 1]]).astype(np.float32)\n",
    "glctx = dr.RasterizeGLContext() #if use_opengl else dr.RasterizeCudaContext()\n",
    "mvp = torch.tensor(np.matmul(projection(x=0.4), translate(0, 0, 0.0)).astype(np.float32), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fdf559-094d-46e5-ae8b-b75122891ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_to_matrix(poses: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert rotations given as quaternions to rotation matrices.\n",
    "\n",
    "    Args:\n",
    "        quaternions: quaternions with real part first,\n",
    "            as tensor of shape (..., 4).\n",
    "\n",
    "    Returns:\n",
    "        Rotation matrices as tensor of shape (..., 3, 3).\n",
    "    \"\"\"\n",
    "    positions = poses[...,:3]\n",
    "    quaternions = poses[...,3:]\n",
    "    r, i, j, k = torch.unbind(quaternions, -1)\n",
    "    x, y, z = torch.unbind(positions, -1)\n",
    "    # pyre-fixme[58]: `/` is not supported for operand types `float` and `Tensor`.\n",
    "    two_s = 2.0 / (quaternions * quaternions).sum(-1)\n",
    "\n",
    "    o = torch.stack(\n",
    "        (\n",
    "            1 - two_s * (j * j + k * k),\n",
    "            two_s * (i * j - k * r),\n",
    "            two_s * (i * k + j * r),\n",
    "            x,\n",
    "            two_s * (i * j + k * r),\n",
    "            1 - two_s * (i * i + k * k),\n",
    "            two_s * (j * k - i * r),\n",
    "            y,\n",
    "            two_s * (i * k - j * r),\n",
    "            two_s * (j * k + i * r),\n",
    "            1 - two_s * (i * i + j * j),\n",
    "            z,\n",
    "            0.0 * x,\n",
    "            0.0 * x,\n",
    "            0.0 * x,\n",
    "            0.0 * x + 1.0,\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    rotation_matrices = o.reshape(quaternions.shape[:-1] + (4, 4))\n",
    "    return rotation_matrices\n",
    "\n",
    "# Transform vertex positions to clip space\n",
    "def transform_pos(mtx, pos):\n",
    "    t_mtx = torch.from_numpy(mtx).cuda() if isinstance(mtx, np.ndarray) else mtx\n",
    "    # (x,y,z) -> (x,y,z,1)\n",
    "    posw = torch.cat([pos, torch.ones([pos.shape[0], 1]).cuda()], axis=1)\n",
    "    return torch.matmul(posw, t_mtx.t())[None, ...]\n",
    "\n",
    "def render(glctx, mtx, pos, pos_idx, resolution: int):\n",
    "    # Setup TF graph for reference.\n",
    "    depth_ = pos[..., 2:3]\n",
    "    depth = torch.tensor([[[(z_val/1)] for z_val in depth_.squeeze()]], dtype=torch.float32).cuda()\n",
    "    pos_clip    = transform_pos(mtx, pos)\n",
    "    rast_out, _ = dr.rasterize(glctx, pos_clip, pos_idx, resolution=[resolution, resolution])\n",
    "    color   , _ = dr.interpolate(depth, rast_out, pos_idx)\n",
    "    # color       = dr.antialias(color, rast_out, pos_clip, pos_idx)\n",
    "    return color\n",
    "    # return rast_out[:,:,:,2:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posevec_to_matrix(position, quat):\n",
    "    return torch.cat(\n",
    "        (\n",
    "            torch.cat((pytorch3d.transforms.quaternion_to_matrix(quat), position.unsqueeze(1)), 1),\n",
    "            torch.tensor([[0.0, 0.0, 0.0, 1.0]],device=device),\n",
    "        ),\n",
    "        0,\n",
    "    )\n",
    "def apply_transform(points, transform):\n",
    "    rels_ = torch.cat(\n",
    "        (\n",
    "            points,\n",
    "            torch.ones((points.shape[0], 1),  device=device),\n",
    "        ),\n",
    "        1,\n",
    "    )\n",
    "    return torch.einsum(\"ij, aj -> ai\", transform, rels_)[...,:3]\n",
    "position = torch.tensor([0.0, 0.1, 0.2], device=device)\n",
    "quat = torch.tensor([1.0, 0.1, 0.2, 0.3],device=device)\n",
    "points = torch.zeros((5,3), device = device)\n",
    "print(apply_transform(points, posevec_to_matrix(position, quat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c2c03-8f55-4625-982d-32479eebfa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(b.utils.get_assets_dir(),\"bop/ycbv/models\")\n",
    "idx = 14\n",
    "mesh_path = os.path.join(model_dir,\"obj_\" + \"{}\".format(idx).rjust(6, '0') + \".ply\")\n",
    "m = b.utils.load_mesh(mesh_path)\n",
    "m = b.utils.scale_mesh(m, 1.0/100.0)\n",
    "\n",
    "m = b.utils.make_cuboid_mesh(jnp.array([0.5, 0.5, 0.2]))\n",
    "\n",
    "vtx_pos = torch.from_numpy(m.vertices.astype(np.float32)).cuda()\n",
    "pos_idx = torch.from_numpy(m.faces.astype(np.int32)).cuda()\n",
    "col_idx = torch.from_numpy(np.zeros((vtx_pos.shape[0],3)).astype(np.int32)).cuda()\n",
    "vtx_col = torch.from_numpy(np.ones((1,3)).astype(np.float32)).cuda()\n",
    "# print(\"Mesh has %d triangles and %d vertices.\" % (pos_idx.shape[0], pos.shape[0]))\n",
    "print(pos_idx.shape, vtx_pos.shape, col_idx.shape, vtx_col.shape)\n",
    "print(vtx_pos, vtx_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_wrapper(pos,quat):\n",
    "    rast_target = render(glctx, torch.matmul(mvp, posevec_to_matrix(pos, quat)), vtx_pos, pos_idx,  resolution)\n",
    "    return rast_target\n",
    "\n",
    "def get_viz(rast_target):\n",
    "    img_target  = rast_target[0].detach().cpu().numpy()\n",
    "    viz = b.get_depth_image(img_target[:,:,0]* 255.0)\n",
    "    return viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f93c7-f7e8-427f-8ea5-365370a9560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = torch.tensor([0.0, 0.0, -2.5],device=device)\n",
    "quat =  torch.tensor(torch.rand(4,device=device) - 0.5,device=device)\n",
    "\n",
    "ground_truth_image = render_wrapper(pos,quat)\n",
    "viz_gt = get_viz(ground_truth_image)\n",
    "viz_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = torch.tensor([0.0, 0.0, -2.2],device=device, requires_grad=True)\n",
    "quat =  torch.tensor(torch.rand(4,device=device) - 0.5,device=device, requires_grad=True)\n",
    "rendered_image = render_wrapper(pos,quat)\n",
    "viz = get_viz(rendered_image)\n",
    "b.hstack_images([viz, viz_gt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5a306-73ca-4981-aca2-1c041ae57f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "    {'params': [pos], 'lr': 0.001, \"name\": \"pos\"},\n",
    "    {'params': [quat], 'lr': 1.0, \"name\": \"quat\"},\n",
    "], lr=0.0, eps=1e-15)\n",
    "print(quat)\n",
    "\n",
    "pbar = tqdm(range(100))\n",
    "for _ in pbar:\n",
    "    rendered_image =  render_wrapper(pos, quat)\n",
    "    loss = torch.abs(ground_truth_image - rendered_image).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_description(f\"{loss.item()}\")\n",
    "viz = get_viz(rendered_image)\n",
    "print(quat)\n",
    "\n",
    "b.hstack_images([viz, viz_gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (rast_opt - rast_target)**2 # L2 norm.\n",
    "diff.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98ecb0-6e55-4280-af4f-6639e5d69f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([pose_opt],  lr=0.00001)\n",
    "images = []\n",
    "\n",
    "for _ in tqdm(range(200)):    \n",
    "    rast_opt = render(glctx, torch.matmul(mvp, quaternion_to_matrix(pose_opt)), vtx_pos, pos_idx,  resolution)\n",
    "\n",
    "    diff = (rast_opt - rast_target)**2 # L2 norm.\n",
    "    loss = torch.mean(diff)\n",
    "    loss_val = float(loss)\n",
    "    \n",
    "    if (loss_val < loss_best) and (loss_val > 0.0):\n",
    "        loss_best = loss_val\n",
    "                \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss)    \n",
    "    with torch.no_grad():\n",
    "        pose_opt /= torch.sum(pose_opt**2)**0.5\n",
    "    \n",
    "    img_opt  = rast_opt[0].detach().cpu().numpy()\n",
    "    images.append(\n",
    "        b.hstack_images([\n",
    "            b.get_depth_image(img_opt[:,:,0]* 255.0) ,\n",
    "            b.get_depth_image(img_target[:,:,0]* 255.0) ,\n",
    "        ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651d8eb-75c3-4453-b58c-09e46ce2b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.vstack_images([images[0],images[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1f6dc-31e1-4c33-97c3-d13fa8e5f34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921bcb5-4d8b-4b0f-8b9f-f3d70ca43522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f58562-0672-4e28-810b-2e30e7c849e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15dc7cd-0aef-44c7-a821-368b512c9fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
